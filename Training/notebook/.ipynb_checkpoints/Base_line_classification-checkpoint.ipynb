{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ancient-liability",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import randint\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from io import StringIO\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-token",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./classification.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-shore",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new dataframe with two column\n",
    "df1 = df[['parent','text']].copy()\n",
    "#remove missing values (NaN)\n",
    "df1 = df1.[pd.notnull(df1['text'])]\n",
    "\n",
    "#renaming second column for a simpler name\n",
    "df1.columns = ['parent','text']\n",
    "\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-making",
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage of complaints with text\n",
    "total = df1['text'].notnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-distance",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df.parent.unique()).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-shannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#because the computation is time consuming (in terms of CPU), the data was sampled\n",
    "df2 = df1.sample(6000,random_state=1).copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-worse",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df2['parent'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-arnold",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['category_id'] = df2['parent'].factorize()[0]\n",
    "category_id_df = df2[['parent', 'category_id']].drop_duplicate()\n",
    "\n",
    "\n",
    "#Dictionaries for future use\n",
    "category_to_id = dict(category_id_df.values)\n",
    "id_to_category = dict(category_id_df[['category_id','parent']].values())\n",
    "\n",
    "#New dataframe\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-joseph",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(sublinear_tf = True,min_df =5,\n",
    "                       ngram_range = (1,2),\n",
    "                       stop_words = 'english')\n",
    "\n",
    "#we transform each complaint into a vector\n",
    "\n",
    "features = tfidf.fit_transform(df2.text).toarray()\n",
    "pickle.dump(features,open('tfidf1.pkl','wb'))\n",
    "labels = df2.category_id\n",
    "\n",
    "print(\"Each of the %d features (TF-IDF score of unigram and bigram)\"%(features.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-realtor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the three most correlated terms with each of the product categories \n",
    "N=3\n",
    "for parent , category_id in sorted(category_to_id.items()):\n",
    "    features_chi2 = chi2(features,label ==category_id)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    features_names = np.array(tfidf.get_features_name())[indices]\n",
    "    unigrams = [v for v in feature_name if len(v.split(' ')) ==1]\n",
    "    bigrams = [v for v in feature_name if len(v.split(' '))==2]\n",
    "    print(\"\\n==> %s:\" %(parent))\n",
    "    print(\" * Most correlated unigrams are: %s\",%(', '.join(unigrams[-N:])))\n",
    "    print(\" * Most correlated unigrams are: %s\",%(', '.join(bigrams[-N:])))\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-turkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    RandomForestClassifier(n_estimators =100,max_depth =5,random_state=0),\n",
    "    LinearSVC(),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state=0)\n",
    "]\n",
    "\n",
    "# 5 cross validation\n",
    "CV=5\n",
    "CV_df = pd.DataFrame(index = range(CV*len(models)))\n",
    "\n",
    "entries = []\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    accuracies = cross_val_score(model,features,labels,scoring = 'accuracy',cv=CV)\n",
    "    for fold_idx,accuracy in enumerate(accuracies):\n",
    "        entries.append(model_name,fold_idx,accuracy)\n",
    "        \n",
    "cv_df = pd.DataFrame(entries,columns = ['model_name','fold_idx','accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-trinity",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accuracy = cv.df.groupby('model_name').accuracy.mean()\n",
    "std_accuracy = cv.df.groupby('model_name').accuracy.std()\n",
    "acc = pd.concat(['Mean Accuracy','Standard Deviation'])\n",
    "acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-metabolism",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(features,labels,df2.index,test_size =0.30,\n",
    "                                                random_state=1)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "from sklearn.extrenals import joblib\\\n",
    "joblib.dump(tfidf, \"./exports/tfidfvectorizerlogisticregression.pkl\")\n",
    "joblib.dump(model, \"./exports/classifierlogisticregression.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-unemployment",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-feature",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('\\t\\t\\t\\t\\t\\tCLASSIFICATION METRICS\\n')\n",
    "print(metrics.classification_report(y_test,y_pred,target_names = df2['parent'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-cassette",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_test,y_pred)\n",
    "fig,ax = plt.subplots(figsize =(8,8))\n",
    "sns.heatmap(conf_mat,annot=True,cmap='Blues',fmt = 'd',\n",
    "           xticklabels = category_id_df.parent.values,\n",
    "           yticklabels = category_id_df.parent.values)\n",
    "\n",
    "plt.ylabel('Actual')\n",
    "plyt.xlabel('Predicted')\n",
    "plt.title('Confusion matrix  -LinearSVC\\n',size = 16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
